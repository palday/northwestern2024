@TechReport{huq:cleland:1990,
  author = 	 {N. M. Huq and J. Cleland},
  title = 	 {Bangladesh Fertility Survey 1989 (Main Report)},
  institution =  {National Institute of Population Research and Training},
  year = 	 1990,
  address = 	 {Dhaka, Bangladesh}}
  address = 	 {Dhaka, Bangladesh}}


@article{fox:effect:2003,
	title = {Effect {Displays} in \textit{{R}} for {Generalised} {Linear} {Models}},
	volume = {8},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v08/i15/},
	doi = {10.18637/jss.v008.i15},
	language = {en},
	number = {15},
	urldate = {2022-08-01},
	journal = {Journal of Statistical Software},
	author = {Fox, John},
	year = {2003},
}

@Article{harper:2016,
  author       = {F. Maxwell Harper and Joseph A. Konstan},
  date         = {2016-01},
  journaltitle = {{ACM} Transactions on Interactive Intelligent Systems},
  title        = {The {MovieLens} Datasets},
  doi          = {10.1145/2827872},
  number       = {4},
  pages        = {1--19},
  volume       = {5},
  publisher    = {Association for Computing Machinery ({ACM})},
}

@article{bates:parsimonious:2018,
	title = {Parsimonious {Mixed} {Models}},
	url = {http://arxiv.org/abs/1506.04967},
	abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.},
	urldate = {2022-04-27},
	journal = {arXiv:1506.04967 [stat]},
	author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
	month = may,
	year = {2018},
	note = {arXiv: 1506.04967},
	keywords = {Statistics - Methodology},
}

@article{alday:baseline:2019,
author = {Alday, Phillip M.},
title = {How much baseline correction do we need in ERP research? Extended GLM model can replace baseline correction while lifting its limits},
journal = {Psychophysiology},
volume = {56},
number = {12},
pages = {e13451},
keywords = {analysis/statistical methods, EEG, ERPs, oscillation/time frequency analyses},
doi = {https://doi.org/10.1111/psyp.13451},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.13451},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.13451},
abstract = {Abstract Baseline correction plays an important role in past and current methodological debates in ERP research (e.g., the Tanner vs. Maess debate in the Journal of Neuroscience Methods), serving as a potential alternative to strong high-pass filtering. However, the very assumptions that underlie traditional baseline also undermine it, implying a reduction in the signal-to-noise ratio. In other words, traditional baseline correction is statistically unnecessary and even undesirable. Including the baseline interval as a predictor in a GLM-based statistical approach allows the data to determine how much baseline correction is needed, including both full traditional and no baseline correction as special cases. This reduces the amount of variance in the residual error term and thus has the potential to increase statistical power.},
year = {2019}
}

@article{brehm:alday:contrasts:2022,
title = {Contrast coding choices in a decade of mixed models},
journal = {Journal of Memory and Language},
volume = {125},
pages = {104334},
year = {2022},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2022.104334},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X22000213},
author = {Laurel Brehm and Phillip M. Alday},
keywords = {Contrasts, Meta-science, Mixed effect models, Replication crisis},
abstract = {Contrast coding in regression models, including mixed-effect models, changes what the terms in the model mean. In particular, it determines whether or not model terms should be interpreted as main effects. This paper highlights how opaque descriptions of contrast coding have affected the field of psycholinguistics. We begin with a reproducible example in R using simulated data to demonstrate how incorrect conclusions can be made from mixed models; this also serves as a primer on contrast coding for statistical novices. We then present an analysis of 3384 papers from the field of psycholinguistics that we coded based upon whether a clear description of contrast coding was present. This analysis demonstrates that the majority of the psycholinguistic literature does not transparently describe contrast coding choices, posing an important challenge to reproducibility and replicability in our field.}
}
